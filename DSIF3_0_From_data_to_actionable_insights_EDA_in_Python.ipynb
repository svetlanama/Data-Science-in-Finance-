{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svetlanama/Data-Science-in-Finance-/blob/main/DSIF3_0_From_data_to_actionable_insights_EDA_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "629795a6",
      "metadata": {
        "id": "629795a6"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this session, we will build on your knowledge from the previous class on data gathering and cleaning. Today, we will dive into exploratory data analysis (EDA) and its importance in the financial sector.\n",
        "We will be using the LendingClub dataset and perform an exploratory data analysis to uncover actionable insights.\n",
        "\n",
        "\n",
        "## Agenda:\n",
        "1. Overview of exploratory data analysis\n",
        "2. Summary statistics - Univariate analysis\n",
        "3. Multivariate analysis\n",
        "4. Enriching your data with feature engineering\n",
        "5. Useful data visualisation techniques\n",
        "6. Data standardisation & Normalisation\n",
        "7. Pandas profiling: a great starting point for EDA\n",
        "8. Assignment #1\n",
        "\n",
        "Demo: Implementation in Python\n",
        "------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "063935b1",
      "metadata": {
        "id": "063935b1"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "335b5fd4",
      "metadata": {
        "id": "335b5fd4"
      },
      "source": [
        "#### User-specified parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9380e912",
      "metadata": {
        "id": "9380e912"
      },
      "outputs": [],
      "source": [
        "python_material_folder_name = \"python-material\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dafeecf8",
      "metadata": {
        "id": "dafeecf8"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c61f7ad7",
      "metadata": {
        "id": "c61f7ad7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# Check if in Google Colab environment\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    # Mount drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Set up path to Python material parent folder\n",
        "    path_python_material = rf\"drive/MyDrive/{python_material_folder_name}\"\n",
        "        # If unsure, print current directory path by executing the following in a new cell:\n",
        "        # !pwd\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    # If working locally on Jupyter Notebook, parent folder is one folder up (assuming you are using the folder structure shared at the beginning of the course)\n",
        "    path_python_material = \"..\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03860b6b",
      "metadata": {
        "id": "03860b6b"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "1\\. Overview of data exploration\n",
        "----------------------------------------\n",
        "Once you have sourced the relevant data for your project, you need to become familiar with it to:\n",
        "- Gain a better understanding of the data's structure and content.\n",
        "- Discover patterns, trends, and relationships within the data.\n",
        "- Extract actionable insights that can inform business decisions.  \n",
        "\n",
        "In summary, this will help you assess how the data can be used to achieve your business objective.\n",
        "\n",
        "---\n",
        "\n",
        "2\\. Summary statistics - Univariate analysis\n",
        "----------------------------------------\n",
        "\n",
        "Univariate analysis involves examining a single variable. Common techniques include calculating central tendency measures like mean, median, and mode.\n",
        "\n",
        "- **Mean**: The average value\n",
        "- **Median**: The middle value\n",
        "- **Mode**: The most frequent value\n",
        "- **Standard Deviation**: Measures the spread of the data\n",
        "- **Percentiles**: Indicate the relative standing of a value in the dataset\n",
        "\n",
        "### Why are they useful?\n",
        "Summary statistics provide a quick overview of the data, allowing us to grasp the general trends and identify any anomalies or outliers.\n",
        "\n",
        "### <span style=\"color:BLUE\"> **>>> QUESTION:** </span>    \n",
        "> *Where have we already encountered descriptive statistics and can you remember how to print some of them out for a pandas dataframe?*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc8fa63",
      "metadata": {
        "id": "cdc8fa63"
      },
      "source": [
        "### Data import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa7d82ee",
      "metadata": {
        "id": "aa7d82ee",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Read data that was exported from previous session\n",
        "df = pd.read_csv(f\"{path_python_material}/data/2-intermediate/df_out_dsif2.csv\")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ea81dc0",
      "metadata": {
        "id": "3ea81dc0"
      },
      "source": [
        "### Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d29bd92b",
      "metadata": {
        "id": "d29bd92b"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import skew\n",
        "\n",
        "# Calculating Summary Statistics\n",
        "mean_loan = df['loan_amnt'].mean()\n",
        "median_loan = df['loan_amnt'].median()\n",
        "mode_loan = df['loan_amnt'].mode()[0]\n",
        "std_loan = df['loan_amnt'].std()\n",
        "percentiles_loan = df['loan_amnt'].quantile([0.25, 0.5, 0.75])\n",
        "skewness_value = skew(df['loan_amnt'])\n",
        "\n",
        "print(f\"Mean Loan Amount: {mean_loan}\")\n",
        "print(f\"Median Loan Amount: {median_loan}\")\n",
        "print(f\"Mode Loan Amount: {mode_loan}\")\n",
        "print(f\"Standard Deviation of Loan Amount: {std_loan}\")\n",
        "print(f\"Loan Amount Percentiles: \\n{percentiles_loan}\")\n",
        "print(\"Skewness:\", skewness_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "811fed41",
      "metadata": {
        "id": "811fed41"
      },
      "source": [
        "### <span style=\"color:BLUE\"> **>>> EXERCISE:**</span>\n",
        "> Create a function that takes as input a dataframe and a column name, and oututs the summary statistics as per the output created in the cell above (mean, median, mode, std deviation and 25th, 50th and 75th percentiles, skewness value.  \n",
        "\n",
        "> Additionally, based on skewness value print the following:  \n",
        "'Skewness = 0: Data is perfectly symmetric.'  \n",
        "'Skewness > 0: Positive skew (right skew).'  \n",
        "'Skewness < 0: Negative skew (left skew).'  \n",
        "\n",
        "> Test the function on `loan_amnt` column first, then on `int_rate` column. What happens when you run it on `int_rate`, can you think of a way around it?   \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b242fca",
      "metadata": {
        "id": "8b242fca"
      },
      "outputs": [],
      "source": [
        "# Step 1 - Create the function\n",
        "def summary_statistics(#FUNCTION PARAMETERS HERE):\n",
        "    \"\"\"\n",
        "    Takes as input a dataframe and a column name, and oututs the following summary statistics: mean, median, mode, std deviation and 25th, 50th and 75th percentiles, skewness value and assessment.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "# Step 2 - Test on loan_amnt\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Step 3 - Test on int_rate\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83ea1293",
      "metadata": {
        "id": "83ea1293"
      },
      "source": [
        "Hint: to improve format and limit number of decimal characters, try adopting the following syntax instead:\n",
        "`mean_loan = (\"%.2f\" % df[col_name].mean())`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ac3eea3",
      "metadata": {
        "id": "9ac3eea3"
      },
      "source": [
        "3\\. Multivariate Analysis\n",
        "----------------------------------------\n",
        "\n",
        "Multivariate analysis involves examining relationships between two or more variables. This can help identify correlations and interactions within the data.\n",
        "\n",
        "### Correlation\n",
        "Correlation is a way to measure how strongly two things are related to each other. It's like asking: \"When one thing changes, does the other thing change too? And if it does, how much?\"\n",
        "\n",
        "**Note**: correlation tells us about the strength and direction of a relationship between two variables (X and Y), but it doesn't give us the exact amount by which Y changes when X changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1266d253",
      "metadata": {
        "id": "1266d253"
      },
      "outputs": [],
      "source": [
        "# Correlation between Loan Amount and Interest Rate\n",
        "correlation = df['loan_amnt'].corr(df['int_rate_clean'])\n",
        "print(f\"Correlation between Loan Amount and Interest Rate: {correlation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2c0ddc5",
      "metadata": {
        "id": "b2c0ddc5"
      },
      "source": [
        "### <span style=\"color:BLUE\"> **>>> EXERCISE:**  </span>\n",
        "> Find the correlation between `loan_amnt` and a new column called `term_numeric` which is a numeric equivalent of the `term` column.    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1bb1609",
      "metadata": {
        "id": "d1bb1609"
      },
      "outputs": [],
      "source": [
        "df['term_numeric'] = # YOUR CODE HERE\n",
        "print(f\"Correlation between Loan Amount and Interest Rate: {# YOUR CODE HERE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b882280",
      "metadata": {
        "id": "8b882280"
      },
      "source": [
        "### <span style=\"color:BLUE\"> **>>> DISCUSSION:**  </span>\n",
        "> What can we conclude?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9346e4ab",
      "metadata": {
        "id": "9346e4ab"
      },
      "source": [
        "4\\. Enriching your data with feature engineering\n",
        "-----------------------\n",
        "\n",
        "### What is Feature Engineering?\n",
        "\n",
        "Feature engineering is the process of using domain knowledge to create new features or modify existing ones to improve the performance of machine learning models.\n",
        "\n",
        "### Why is it important?\n",
        "\n",
        "Feature engineering can significantly enhance the predictive power of models by incorporating additional information or transforming existing data into more useful formats.\n",
        "\n",
        "### Techniques\n",
        "\n",
        "-   **Creating New Features**: e.g., debt-to-income ratio\n",
        "-   **Transforming Existing Features**: e.g., log transformation\n",
        "-   **Feature encoding**: e.g.: dummy encoding, label encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4637637e",
      "metadata": {
        "id": "4637637e"
      },
      "outputs": [],
      "source": [
        "print(*df.columns, sep=(\"\\n\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d299b4cf",
      "metadata": {
        "id": "d299b4cf"
      },
      "source": [
        "## Creating new features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f20195c",
      "metadata": {
        "id": "5f20195c"
      },
      "outputs": [],
      "source": [
        "# Creating a new feature: debt-to-income ratio\n",
        "df['debt_to_income'] = df['loan_amnt'] / df['annual_inc']\n",
        "df[['id','loan_amnt', 'annual_inc', 'debt_to_income']].head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a8c8e0c",
      "metadata": {
        "id": "3a8c8e0c"
      },
      "source": [
        "### <span style=\"color:BLUE\"> **>>> EXERCISE:**  </span>\n",
        "> Let's create a new feature called `interest_per_loan_amnt`, which calculates the total interest paid throughout the course of the term based on interest rate, loan amount and term information.   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f773d47",
      "metadata": {
        "id": "6f773d47"
      },
      "outputs": [],
      "source": [
        "df['interest_per_loan_amnt'] = # YOUR CODE HERE\n",
        "\n",
        "# Printing out results\n",
        "df[['id','loan_amnt', 'int_rate_clean', 'term_numeric', 'interest_per_loan_amnt']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97af0dff",
      "metadata": {
        "id": "97af0dff"
      },
      "source": [
        "## Transforming Existing Features\n",
        "### Example: Log transformation\n",
        "Log transformation is a technique used to handle skewed data. It involves applying the natural logarithm (or another logarithm) to the data to reduce skewness and stabilize variance.\n",
        "\n",
        "**Why is this important?**\n",
        "Skewed data can distort statistical analyses and machine learning models, leading to poor predictions. Log transformation can help normalize the data, making it more suitable for analysis.\n",
        "Many financial variables, such as income or loan amounts, can have long-tailed distributions. By applying log transformations, we ensure that these features are better suited for linear models and improve the modelâ€™s performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf8b169",
      "metadata": {
        "id": "caf8b169"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Log transformation\n",
        "df['loan_amnt_log'] = np.log(df['loan_amnt'] + 1) # Adding +1 to avoid log(0)\n",
        "\n",
        "# Plotting original and log-transformed data\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(df['loan_amnt'], bins=10, color='blue', edgecolor='black')\n",
        "plt.title('Original data')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(df['loan_amnt_log'], bins=10, color='green', edgecolor='black')\n",
        "plt.title('Log-transformed data')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(f\"Skewness before log transform: {skew(df['loan_amnt'])}\")\n",
        "print(f\"Skewness after log transform: {skew(df['loan_amnt_log'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c502cc6f",
      "metadata": {
        "id": "c502cc6f"
      },
      "source": [
        "## Feature encoding\n",
        "**Categorical feature encoding** is a crucial step in preparing categorical data for use in machine learning models. Categorical data includes variables that represent discrete categories or groups, such as \"loan grade,\" \"employment status,\" or \"credit rating.\" Unlike numerical data, these categories do not have an inherent order or scale that algorithms can directly interpret. Therefore, we need to convert these categories into a numerical format that can be utilized by machine learning models.\n",
        "\n",
        "### Dummy Encoding (One-Hot Encoding)\n",
        "\n",
        "**Dummy Encoding** (or One-Hot Encoding) converts each category of a categorical variable into a new binary column (0 or 1). Each column represents a category, with a value of `1` indicating the presence of that category in the observation, and `0` otherwise.\n",
        "\n",
        "**Why is this important?**\n",
        "Machine learning algorithms generally require numerical input. Without converting categorical data into numerical form, algorithms wouldn't be able to process the data, leading to errors.\n",
        "\n",
        "By using dummy encoding, you preserve the distinct nature of each category without implying any order or rank between them. This is especially important for models like linear regression, where numerical values can suggest a relationship or order that does not actually exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7118387f",
      "metadata": {
        "id": "7118387f"
      },
      "outputs": [],
      "source": [
        "# One-hot encoding\n",
        "df_dummies = pd.get_dummies(df['grade'], prefix='grade')\n",
        "df_dummies.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89f0949c",
      "metadata": {
        "id": "89f0949c"
      },
      "source": [
        "### Label Encoding\n",
        "Label Encoding assigns a unique integer to each category in the categorical variable. For example, a \"loan grade\" of 'A' might be encoded as `0`, 'B' as `1`, and so on.\n",
        "\n",
        "**Why is this important?**: Label encoding is useful when the categorical variable is ordinal (i.e., there is a meaningful order or ranking to the categories). However, for non-ordinal data, it might imply an order that does not exist, potentially misleading the model.\n",
        "This method is simple and works well when you need to convert a categorical variable with many levels into a numerical format quickly. However, be careful not to unintentionally introduce ordinal relationships when none exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b9dea5",
      "metadata": {
        "id": "08b9dea5"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Label encoding\n",
        "le = LabelEncoder()\n",
        "df['grade_encoded'] = le.fit_transform(df['grade'])\n",
        "df[['grade', 'grade_encoded']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f95bb8e1",
      "metadata": {
        "id": "f95bb8e1"
      },
      "source": [
        "5\\. Useful data visualisation techniques\n",
        "----------------------\n",
        "### Why is Visualization Important?\n",
        "\n",
        "Visualization helps in understanding data patterns and communicating findings effectively. Common visualization techniques include:\n",
        "\n",
        "-   **Histograms**: To visualize distributions\n",
        "-   **Box Plots**: To identify outliers\n",
        "-   **Bar Plots**: To compare categories\n",
        "-   **Pair Plots**: To explore relationships between pairs of variables\n",
        "-   **Heatmaps**: To visualize correlations\n",
        "-   **Violin Plots**: To combine density and summary statistics\n",
        "\n",
        "Check out the [seaborn cheat sheet](https://www.datacamp.com/cheat-sheet/python-seaborn-cheat-sheet) for more info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d554a314",
      "metadata": {
        "id": "d554a314"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Histogram of Loan Amount\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(df['loan_amnt'], kde=True)\n",
        "plt.title('Distribution of Loan Amount')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b892a48",
      "metadata": {
        "id": "3b892a48"
      },
      "outputs": [],
      "source": [
        "# Box Plot of Interest Rate\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(y=df['int_rate_clean'])\n",
        "plt.title('Box Plot of Interest Rate',)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df13d033",
      "metadata": {
        "id": "df13d033"
      },
      "outputs": [],
      "source": [
        "# Select numeric columns only\n",
        "df_numeric = df.select_dtypes(exclude=\"object\")\n",
        "\n",
        "# Heatmap of Correlations\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df_numeric.corr(), annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbfafc73",
      "metadata": {
        "id": "bbfafc73"
      },
      "source": [
        "Hard to read? Absolutely, this is because too many columns are selected. Let's try on a subset of columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "774b63ab",
      "metadata": {
        "id": "774b63ab"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df_numeric \\\n",
        "            .drop(columns=df_numeric.columns[10:]) \\\n",
        "            .corr(), annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1df1f3d9",
      "metadata": {
        "id": "1df1f3d9"
      },
      "outputs": [],
      "source": [
        "df_numeric.corr().head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b7932cd",
      "metadata": {
        "id": "4b7932cd"
      },
      "outputs": [],
      "source": [
        "# How to use or interpret?\n",
        "\n",
        "df_to_iterate = df_numeric.corr()\n",
        "\n",
        "# Iterate through each numeric column in the dataframe\n",
        "for column in df_to_iterate.columns:\n",
        "    # Sort the values in descending order and take the top 3\n",
        "    top_3_values = df_to_iterate[column].nlargest(3)\n",
        "    print(f\"Top 3 values for column '{column}':\\n{top_3_values}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f049f4a",
      "metadata": {
        "id": "5f049f4a"
      },
      "source": [
        "Key highlights:\n",
        "- High correlation (red) is expected across the diagonal (unsurprisingly, features are correlated to themselves..)\n",
        "- It is important to review features that are correlated and ask ourselves 2 key questions:  \n",
        "    > a) Do we understand why they are, in other words is it expected?  \n",
        "    > b) Do we think a feature may carry additional information to the features it shows high correlation with? If so, it may be worth retaining for modelling purposes, *however* we want to minimise the risk of multicollinearity, which poses a problem for some types of models.  \n",
        "\n",
        "We will be talking about this in our modelling classes, so don't worry too much about it for now."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb029dc5",
      "metadata": {
        "id": "cb029dc5"
      },
      "source": [
        "### <span style=\"color:BLUE\"> **>>>EXERCISE:**  </span>\n",
        "Looks like there may be some correlation between income and loan amount (expectedly, as higher income individuals are able to afford higher loan amounts).\n",
        "    \n",
        "Search the documentation of [seaborn](seaborn.com) and create a pair plot between those 2 features     \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d3b9d76",
      "metadata": {
        "id": "8d3b9d76",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a51206c8",
      "metadata": {
        "id": "a51206c8"
      },
      "source": [
        "6\\. Data standardisation & Normalisation\n",
        "----------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f0b7e6",
      "metadata": {
        "id": "24f0b7e6"
      },
      "source": [
        "### Data standardization\n",
        "Standardization involves transforming data so that it has a mean of zero and a standard deviation of one. This is particularly important for algorithms that assume data is normally distributed and those that are sensitive to the scale of data, such as:  \n",
        "1\\. **Gradient descent-based Algorithms:** Algorithms like linear regression, logistic regression, and neural networks.  \n",
        "2\\. **Distance-based Algorithms:** Algorithms such as K-nearest neighbors (KNN) and K-means clustering, which rely on distance metrics.\n",
        "\n",
        "### Data normalization\n",
        "Normalization scales data to a range of [0, 1] or [-1, 1]. It's crucial for:  \n",
        "1\\. **Ensuring Uniform Contribution:** Each feature contributes equally to the model.  \n",
        "2\\. **Neural Networks:** Where the range of input data can significantly impact the training process.  \n",
        "\n",
        "Standardization is generally preferred when working with algorithms that assume normally distributed data or when features have different units and you need them to have zero mean and unit variance.  \n",
        "Normalization is typically used when you need to scale features to a fixed range or when working with algorithms sensitive to the magnitude of input data.  \n",
        "\n",
        "Let's take a look at the distribution of two numerical features: `loan_amnt` (loan amount) and `annual_inc` (annual income).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35880230",
      "metadata": {
        "id": "35880230"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "df[['loan_amnt_std', 'annual_inc_std']] = scaler.fit_transform(df[['loan_amnt', 'annual_inc']])\n",
        "\n",
        "# Plotting the distributions before standardization\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['loan_amnt'], bins=30, kde=True)\n",
        "plt.title('Loan Amount Distribution (Before Standardization)')\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df['annual_inc'], bins=30, kde=True)\n",
        "plt.title('Annual Income Distribution (Before Standardization)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plotting the distributions after standardization\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['loan_amnt_std'], bins=30, kde=True)\n",
        "plt.title('Loan Amount Distribution (After Standardization)')\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df['annual_inc_std'], bins=30, kde=True)\n",
        "plt.title('Annual Income Distribution (After Standardization)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6363f487",
      "metadata": {
        "id": "6363f487"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Normalizing the data\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "df[['loan_amnt_norm', 'annual_inc_norm']] = scaler.fit_transform(df[['loan_amnt', 'annual_inc']])\n",
        "\n",
        "# Plotting the distributions after normalization\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['loan_amnt_norm'], bins=30, kde=True)\n",
        "plt.title('Loan Amount Distribution (After Normalization)')\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df['annual_inc_norm'], bins=30, kde=True)\n",
        "plt.title('Annual Income Distribution (After Normalization)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ed2e5fe",
      "metadata": {
        "id": "7ed2e5fe"
      },
      "source": [
        "### Use Cases in Lending\n",
        "\n",
        "#### Credit Risk Analysis\n",
        "Standardization and normalization can help improve the accuracy of credit risk models by ensuring that numerical features are on a similar scale, preventing any single feature from disproportionately influencing the model.\n",
        "\n",
        "#### Fraud Detection\n",
        "Standardizing and normalizing transaction amounts can help identify anomalies or unusual patterns that may indicate fraudulent activities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93464cf5",
      "metadata": {
        "id": "93464cf5"
      },
      "source": [
        "7\\. Pandas profiling: a great starting point for EDA\n",
        "-------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f7506e4",
      "metadata": {
        "id": "0f7506e4"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68601460",
      "metadata": {
        "id": "68601460"
      },
      "outputs": [],
      "source": [
        "# Sampling data for ease of processing (note: make sure the sample size is large enough to extrapolate from)\n",
        "features = ['term', 'fico_range_high', 'fico_range_low', 'annual_inc', 'dti']\n",
        "df_sampled = df[features].sample(100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e23e8862",
      "metadata": {
        "id": "e23e8862"
      },
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "report = ProfileReport(df_sampled, title= \"Report\")\n",
        "report # Will take a few minutes to process\n",
        "\n",
        "# report.to_file('../reports/dsif3-data-profiling.html') # To save as html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22a1dbf5",
      "metadata": {
        "id": "22a1dbf5"
      },
      "source": [
        "## Data export\n",
        "Before moving to the last section on time series, let's export our data to the \"intermediate\" data folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43598276",
      "metadata": {
        "id": "43598276"
      },
      "outputs": [],
      "source": [
        "df.to_csv(f\"{path_python_material}/data/2-intermediate/df_out_dsif3.csv\"\n",
        "                        , index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f19d0b02",
      "metadata": {
        "id": "f19d0b02"
      },
      "source": [
        "8\\. Assignment #1\n",
        "-------------\n",
        "\n",
        "Take your data cleaning and exploratory data analysis further and uncover **at least 2 additional actionable insights** as an outcome of EDA.\n",
        "Tips:\n",
        "- Feel free to drop or generate additional features in doing so.\n",
        "- How can the business leverage the insights uncovered and how can the impact of your insight be measured?\n",
        "\n",
        "**Submission:** Submit your notebook with the EDA and insights documented.\n",
        "\n",
        "**Evaluation Criteria:**\n",
        "\n",
        "-   Depth of analysis and insights derived.\n",
        "-   Relevance and feasibility of the actionable insights.\n",
        "-   Clarity and thoroughness of code and explanations.\n",
        "\n",
        "Tips: don't be afraid to be creative, and go past what was covered in this session (e.g. additional plot types etc.)\n",
        "\n",
        "Happy exploring!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85131319",
      "metadata": {
        "id": "85131319"
      },
      "outputs": [],
      "source": [
        "#### Example structure ####\n",
        "\n",
        "# 1. Load data\n",
        "#    > recommended to use output from above class\n",
        "\n",
        "# 2. Data cleaning\n",
        "\n",
        "# 3. Summary statistics\n",
        "\n",
        "# 4. Feature engineering\n",
        "\n",
        "# 5. Visualization\n",
        "#    > check out seaborn documentation at link above\n",
        "\n",
        "# 6. Insights and conclusions\n",
        "#    > what actions is your insight driving\n",
        "#    (could be data cleaning action, or a question/hypothesis for the business to test, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c722064b",
      "metadata": {
        "id": "c722064b"
      },
      "source": [
        "# End of session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a06b6644",
      "metadata": {
        "id": "a06b6644"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(filename='../images/the-end.jpg', width=500,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47f2cdc2",
      "metadata": {
        "id": "47f2cdc2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "dsif",
      "language": "python",
      "name": "dsif"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}