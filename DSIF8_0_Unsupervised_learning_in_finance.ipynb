{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svetlanama/Data-Science-in-Finance-/blob/main/DSIF8_0_Unsupervised_learning_in_finance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH1XQ07F5Bx8"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "\n",
        "## Agenda:\n",
        "**1. Outlier detection**  \n",
        "**2. Clustering**  \n",
        "**3. Principal Component Analysis**  \n",
        "**4. Assignment**  \n",
        "**5. Bonus content - NLP and sentiment analysis**  \n",
        "\n",
        "\n",
        "Demo: Implementation in Python\n",
        "------------------------------\n",
        "\n",
        "### LendingClub Use Case\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQcNIIZhTJDJ"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jniugz5aTJDO"
      },
      "source": [
        "#### User-specified parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWUuadE6TJDR"
      },
      "outputs": [],
      "source": [
        "python_material_folder_name = \"python-material-INSTRUCTOR\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdOsQ4-JTJDU"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d26GbnSyTJDV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# Check if in Google Colab environment\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    # Mount drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Set up path to Python material parent folder\n",
        "    path_python_material = rf\"drive/MyDrive/{python_material_folder_name}\"\n",
        "        # If unsure, print current directory path by executing the following in a new cell:\n",
        "        # !pwd\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    # If working locally on Jupyter Notebook, parent folder is one folder up (assuming you are using the folder structure shared at the beginning of the course)\n",
        "    path_python_material = \"..\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi2ZAw77TJDX"
      },
      "source": [
        "#### Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-Uk31wyTJDX",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Read data that was exported from previous session\n",
        "df = pd.read_csv(f\"{path_python_material}/data/2-intermediate/df_out_dsif6.csv\").sample(1000)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGODy-ZCTJDY"
      },
      "source": [
        "# 1 - Outlier detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKFEVqKcTJDZ"
      },
      "source": [
        "Do you remember where we saw these in previous lessons? and what methodology we used?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjJtbTcITJDZ"
      },
      "source": [
        "## 1.1 - Z scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yh5pJ6kTJDa"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Calculate the Z-score for a specific column (e.g., annual_inc)\n",
        "df['annual_inc_zscore'] = np.abs(stats.zscore(df['annual_inc']))\n",
        "\n",
        "# Flag outliers (Z-score > 3)\n",
        "outliers = df[df['annual_inc_zscore'] > 3]\n",
        "print(f\"Outliers detected using z-scores: {len(outliers)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mfuDSz1TJDa"
      },
      "source": [
        "## 1.2 - Interquantile range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR574QKlTJDa"
      },
      "outputs": [],
      "source": [
        "df_sample = df.sample(100) # for viz purposes\n",
        "\n",
        "# Calculate IQR for annual_inc\n",
        "Q1 = df['annual_inc'].quantile(0.25)\n",
        "Q3 = df['annual_inc'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define outlier boundaries\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Identify outliers\n",
        "outliers_iqr = df_sample[(df_sample['annual_inc'] < lower_bound) | (df_sample['annual_inc'] > upper_bound)]\n",
        "print(f\"Outliers detected using IQR: {len(outliers_iqr)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJBjf6byTJDa"
      },
      "outputs": [],
      "source": [
        "# Also this is the same as using a Box plot\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Box Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(y=df_sample['annual_inc'])\n",
        "plt.title('Box plot of annual_inc')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw43OUFMTJDb"
      },
      "source": [
        "## 1.3 - Autoencoders - (Bonus content)\n",
        "\n",
        "Intuition: By training an autoencoder on normal data and calculating reconstruction errors on test data, we can identify outliers as those with reconstruction errors significantly higher than the threshold.\n",
        "\n",
        "Example [here](https://www.kaggle.com/code/robinteuwens/anomaly-detection-with-auto-encoders).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY6BG3AxTJDe"
      },
      "source": [
        "# 2. Clustering\n",
        "## 2.1 k-means clustering\n",
        "popular clustering algorithm that partitions the data into `k` clusters by minimizing the variance within each cluster.\n",
        "\n",
        "**Note:** We will be using 2 features in the example as it is much easier to visualise and get acquainted with this technique, but where clustering becomes very useful is on multi-dimentional data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIYXCnarTJDe"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Let's use a simplified dataset for demonstration (e.g., annual income and loan amount)\n",
        "data_for_clustering = df[['annual_inc', 'loan_amnt']].dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vZBef2D7ZcQ"
      },
      "outputs": [],
      "source": [
        "# Let's use the Elbow graph to determine optimal number of clusters:\n",
        "wcss = []\n",
        "max_clusters = 10\n",
        "\n",
        "for i in range(1, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
        "    kmeans.fit(data_for_clustering)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Elbow Graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x=range(1, max_clusters + 1), y=wcss, marker='o', linestyle='--')\n",
        "plt.xlabel('# of clusters')\n",
        "plt.ylabel('Within-cluster sum of squares')\n",
        "plt.title('Elbow method for optimal number of clusters')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYHB4_LrTJDr"
      },
      "source": [
        "The \"elbow point\" on this graph indicates the optimal number of clusters, i.e. where adding more clusters doesn't significantly reduce WCSS.\n",
        "\n",
        "Based on the above chart, the 'elbow' point seems to be at around 3 clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2ZK8Ae07ZcQ"
      },
      "outputs": [],
      "source": [
        "num_clusters = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KH1Py7Ef7ZcQ"
      },
      "outputs": [],
      "source": [
        "# Applying K-Means\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "data_for_clustering['cluster-km'] = kmeans.fit_predict(data_for_clustering)\n",
        "data_for_clustering.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVT4HW-vTJDq"
      },
      "outputs": [],
      "source": [
        "# Visualize the clusters\n",
        "plt.scatter(data_for_clustering['annual_inc'], data_for_clustering['loan_amnt'], c=data_for_clustering['cluster-km'], cmap='viridis')\n",
        "plt.xlabel('Annual Income')\n",
        "plt.ylabel('Loan Amount')\n",
        "plt.title('K-Means Clustering')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qgwjthvTJDr"
      },
      "source": [
        "### Extrapolating insight from the created clusters\n",
        "Extrapolating insights from the created clusters involves analyzing the characteristics of each group (and relationship between them) to uncover patterns that can drive business decisions.\n",
        "\n",
        "By understanding the defining features of each cluster (e.g. spending habits, income levels, risk profiles etc.) you can craft a compelling business narrative that highlights how different customer segments behave and what they value.  \n",
        "\n",
        "This can be leveraged to align business strategies with customer needs, tailor marketing campaigns, optimize product offerings, and enhance customer engagement. Ultimately, translating these insights into a coherent story enables stakeholders to grasp the significance of the data, make informed decisions, and identify new opportunities for growth and innovation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HrRjF6PcDRy"
      },
      "outputs": [],
      "source": [
        "data_for_clustering.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZmUhXIsTJDs"
      },
      "outputs": [],
      "source": [
        "# Provide key Statistics by cluster\n",
        "cluster_summary = data_for_clustering\\\n",
        "    .groupby('cluster-km').agg(['mean'])#, 'median', 'std', 'min', 'max'])\n",
        "print(\"Key stats by cluster:\")\n",
        "\n",
        "# Set float format to display with 2 decimal places\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "cluster_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGdsT6vz7ZcQ"
      },
      "outputs": [],
      "source": [
        "# Visualize cluster means for each feature\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "feats=list(data_for_clustering.drop(columns = ['cluster-km',]).columns)\n",
        "print(feats)\n",
        "\n",
        "for feature in feats:\n",
        "    sns.barplot(x='cluster-km', y=feature, data=data_for_clustering, estimator=np.mean, ci=None)\n",
        "    plt.title(f'Mean of {feature} by Cluster')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlOopQSK7ZcQ"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(data_for_clustering[['loan_amnt', 'annual_inc', 'cluster-km']]\n",
        "             ,hue = 'cluster-km')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tANepQhQTJDs"
      },
      "source": [
        "## 2.2 - Hierarchical clustering\n",
        "Builds a hierarchy of clusters either in a bottom-up approach (agglomerative) or top-down approach (divisive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHT5A6-rTJDs"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Generate the linkage matrix\n",
        "linked = linkage(data_for_clustering[['annual_inc', 'loan_amnt']]\n",
        "                , method='ward')\n",
        "\n",
        "# First define the leaf label function.\n",
        "def llf(id):\n",
        "    if id < n:\n",
        "        return str(id)\n",
        "    else:\n",
        "        return '[%d %d %1.2f]' % (id, count, R[n-id,3])\n",
        "\n",
        "\n",
        "# Plot the dendrogram\n",
        "plt.figure(figsize=(10, 7))\n",
        "dendrogram(linked\n",
        "           ,truncate_mode=\"level\" ,p= 7\n",
        "          )\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('Data Points or Cluster Labels')\n",
        "plt.ylabel('Distance (Euclidean)')\n",
        "plt.xticks()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGOt-3pHTJDt"
      },
      "source": [
        "**What does this show?**  \n",
        "The y-axis of a dendrogram represents the distance or similarity between clusters when they are merged.\n",
        "We selected 'ward' method, hence distance metric used in this case is the variance increase (within-cluster) when clusters are merged.\n",
        "\n",
        "**So what?**  \n",
        "By analyzing the y-axis, which represents the distance or dissimilarity between merged clusters, you can determine the point at which clusters should stop being merged to retain meaningful groupings in your data.\n",
        "By drawing an imaginatory horizontal line, we can pick the 'optimal number of clusters', i.e. when clusters start to become similar (low distance on the y-axis) to each other.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--oVvbqtTJDt"
      },
      "source": [
        "### <span style=\"color:BLUE\"> **>>> DISCUSSION:**  </span>     \n",
        "How many clusters should we opt for in this case?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opm9joCRTJDt"
      },
      "source": [
        "## 2.3 - DBSCAN (Bonus content)\n",
        "DBSCAN stands for Density-Based Spatial Clustering of Applications with Noise.  \n",
        "DBSCAN clusters data points that are close to each other based on density, with points in low-density regions considered as noise (outliers).  \n",
        "\n",
        "**Advantages**\n",
        "- Does not require specifying the number of clusters upfront\n",
        "- Can find arbitrarily shaped clusters (see slides)\n",
        "- Effectively identifies outliers\n",
        "\n",
        "**Disadvantages**\n",
        "- Sensitive to the choice of parameters (epsilon and min_samples) - see below!\n",
        "> **eps (epsilon)**: The maximum distance between two samples for them to be considered as in the same neighborhood.  \n",
        "> **min_samples**: The minimum number of samples required in a neighborhood to form a core point (a point that can s\n",
        "- Expects data to be standardised or normalised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77UoT6UnTJDu"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "epsilon_range = range(1, 10, 2)\n",
        "min_samples_range = range(1, 10, 2)\n",
        "\n",
        "for epsilon in epsilon_range:\n",
        "    for minimum_samples in min_samples_range:\n",
        "\n",
        "        if epsilon == 0:  # DBSCAN requires epsilon > 0, so skip this\n",
        "            continue\n",
        "\n",
        "        # Apply DBSCAN with the current pair of parameters\n",
        "        dbscan = DBSCAN(eps=epsilon, min_samples=minimum_samples)\n",
        "        data_for_clustering['cluster-dbscan'] = dbscan.fit_predict(data_for_clustering[['annual_inc', 'loan_amnt']])\n",
        "\n",
        "        # Count and print the number of clusters (excluding noise labeled as -1)\n",
        "        n_clusters = len(set(data_for_clustering['cluster-dbscan']) - {-1})\n",
        "        print(f\"eps={epsilon}, min_samples={minimum_samples}: Number of clusters identified = {n_clusters}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Sry28aY7ZcR"
      },
      "source": [
        "To determine a good starting value for the eps parameter in DBSCAN, you can use a K-nearest neighbors (KNN) distance plot. This plot helps you visualize the distance to the nearest neighbors and identify a suitable eps value where the slope of the curve changes significantly (often called the \"elbow point\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2su8-j_k7ZcR"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Select relevant features for clustering\n",
        "X = data_for_clustering[['annual_inc', 'loan_amnt']].values\n",
        "\n",
        "# Fit Nearest Neighbors model to the data (for k = 4 because DBSCAN typically considers 4 nearest neighbors)\n",
        "neighbors = NearestNeighbors(n_neighbors=4)\n",
        "neighbors_fit = neighbors.fit(X)\n",
        "\n",
        "# Calculate distances and sort them\n",
        "distances, indices = neighbors_fit.kneighbors(X)\n",
        "distances = np.sort(distances[:, 3])  # Sort distances to the 4th nearest neighbor\n",
        "\n",
        "# Plot KNN distance plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(distances)\n",
        "plt.title('KNN Distance Plot')\n",
        "plt.xlabel('Data Points sorted by distance')\n",
        "plt.ylabel('4th Nearest Neighbor Distance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtGtUzlz7ZcR"
      },
      "outputs": [],
      "source": [
        "epsilon = 950\n",
        "min_samples_range = range(1, 10, 1)\n",
        "\n",
        "for minimum_samples in min_samples_range:\n",
        "\n",
        "    # Apply DBSCAN with the current pair of parameters\n",
        "    dbscan = DBSCAN(eps=epsilon, min_samples=minimum_samples)\n",
        "    data_for_clustering['cluster-dbscan'] = dbscan.fit_predict(data_for_clustering[['annual_inc', 'loan_amnt']])\n",
        "\n",
        "    # Count and print the number of clusters (excluding noise labeled as -1)\n",
        "    n_clusters = len(set(data_for_clustering['cluster-dbscan']) - {-1})\n",
        "    print(f\"eps={epsilon}, min_samples={minimum_samples}: Number of clusters identified = {n_clusters}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7omBga77ZcR"
      },
      "outputs": [],
      "source": [
        "chosen_min_samples = 8\n",
        "dbscan = DBSCAN(eps=epsilon, min_samples=chosen_min_samples)\n",
        "data_for_clustering['cluster-dbscan'] = dbscan.fit_predict(data_for_clustering)\n",
        "print(f\"Number of clusters: {len(data_for_clustering['cluster-dbscan'].unique())-1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xsy8g3qz7ZcR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define unique cluster labels\n",
        "unique_labels = data_for_clustering['cluster-dbscan'].unique()\n",
        "\n",
        "# Create a color map for categorical clusters\n",
        "colors = sns.color_palette('plasma', len(unique_labels))\n",
        "# # Manually specify distinct colors for each cluster\n",
        "# colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink']  # Add more if needed\n",
        "\n",
        "# Create a dictionary to map cluster labels to colors\n",
        "color_map = dict(zip(unique_labels, colors))\n",
        "\n",
        "# Create scatter plot with categorical colors\n",
        "plt.figure(figsize=(10, 6))\n",
        "for label in unique_labels:\n",
        "    plt.scatter(data_for_clustering[data_for_clustering['cluster-dbscan'] == label]['annual_inc'],\n",
        "                data_for_clustering[data_for_clustering['cluster-dbscan'] == label]['loan_amnt'],\n",
        "                color=color_map[label],\n",
        "                label=f'Cluster {label}')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Annual Income')\n",
        "plt.ylabel('Loan Amount')\n",
        "plt.title('DBSCAN Clustering: Annual Income vs Loan Amount')\n",
        "\n",
        "# Add legend for categorical clusters\n",
        "plt.legend(title='Clusters')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Iw27GMK7ZcR"
      },
      "source": [
        "If your DBSCAN is identifying a large number of outliers and very few clusters, it typically means that the eps parameter is too small or the min_samples parameter is too large.\n",
        "\n",
        "Few things you can do to adjust:  \n",
        "- Increase epsilon and decrease min_sample params  \n",
        "- Check data scaling: make sure the data has been standardized or normalized  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7HcKGGSTJEM"
      },
      "source": [
        "# 3 - Principal Component Analysis (PCA)\n",
        "Principal Component Analysis (PCA) is a technique for dimensionality reduction and feature extraction that can be used to simplify complex datasets, identify patterns, and improve model performance.\n",
        "\n",
        "\n",
        "## 3.1 - Applications\n",
        "\n",
        "PCA is often used to:\n",
        "- reduce dimensionality in the data - in particular the **curse of dimensionality** refers to challenges that highly-dimensional data leads to, such as overfitting, increased computational cost, and difficulty in visualisation/interpretability. Principal Component Analysis (PCA) is a technique that helps mitigate these problems by reducing the number of dimensions while retaining most of the variability in the data.\n",
        "- for segmentation purposes: PCA can be helpful to visualise segments, having reduced multi dimensional data into lower dimensional space (e.g. 2 PCs).\n",
        "\n",
        "\n",
        "## 3.2 - How does it work?\n",
        "\n",
        "PCA is a linear transformation technique that projects data into a new coordinate system. The new coordinates, called **principal components**, are ordered by the amount of variance they capture from the data. We say that the new features that are **linear combinations** of the original features.\n",
        "\n",
        "**Steps followed:**\n",
        "1. **Standardize the Data:** Center the data around the mean and scale it to unit variance.\n",
        "2. **Compute the Covariance Matrix:** Measure the covariance between features.\n",
        "3. **Calculate Eigenvalues and Eigenvectors:** Determine the principal components.\n",
        "4. **Sort and Select Principal Components:** Choose components that capture the most variance.\n",
        "5. **Transform the Data:** Project data onto the selected principal components.\n",
        "\n",
        "More details [here](https://en.wikipedia.org/wiki/Principal_component_analysis).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mcPA7pj7ZcR"
      },
      "source": [
        "## 3.3. Python implementation\n",
        "\n",
        "In this section, we will see a very common workflow consisting of:\n",
        "- Running PCA\n",
        "- Then running analysis on top of PCA outputs, more specifically:\n",
        "    - Outlier detection\n",
        "    - Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NsJG2_z7ZcR"
      },
      "source": [
        "### 3.3.1 - PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGgMPU6MTJEM"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHmUux5z7ZcR"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB92zXRxTJEM"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df)\n",
        "len(df_scaled), len(df_scaled[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDy0Ge0YTJEM"
      },
      "outputs": [],
      "source": [
        "# Initialize PCA with the number of components you want to keep\n",
        "pca = PCA(n_components=2)  # Reduce to 2 dimensions, often used for visualization\n",
        "df_pca = pd.DataFrame(pca.fit_transform(df_scaled)\n",
        "                      , columns = [\"pc1\", \"pc2\"])\n",
        "\n",
        "# Explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"Explained variance ratio by component:\", explained_variance)\n",
        "\n",
        "# Cumulative explained variance\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "print(\"Cumulative explained variance:\", cumulative_variance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wORegmObTJEN"
      },
      "source": [
        "**Explained Variance Ratio** indicates the proportion of the dataset's total variance that is captured by each principal component.  \n",
        "**Cumulative Explained Variance** is the sum of the explained variance ratios up to a given principal component, i.e. total proportion of the variance explained by the first *'X'* principal components.\n",
        "\n",
        "**Caveat:** 23% not good enough to use these principal components alone, we would need to add a few more to get to 80%+ (rule of thumb) to use PCs instead of original data. Sticking to 2 here for explainability purposes.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwullFPuTJEN"
      },
      "outputs": [],
      "source": [
        "# Plot the transformed data\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df_pca[\"pc1\"], df_pca[\"pc2\"], alpha=0.7)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA - 2D Projection of the Data')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chi-Gp0pTJEN"
      },
      "source": [
        "PCA can be useful in conjunction with clustering techniques as a visualisation tool.\n",
        "\n",
        "By projecting high-dimensional data into a 2D or 3D space, PCA allows the different clusters identified to be visualized in a way that is easy to interpret. This helps in validating whether the clusters are well-separated or overlapping, providing a clearer understanding of the underlying data structure.\n",
        "\n",
        "From a business perspective, this visualization aids in making data-driven decisions. For instance, if a company segments its customers into different clusters based on purchasing behavior, PCA can help visualize these segments, revealing insights about customer groups that might not be immediately obvious. This can lead to more targeted marketing strategies, better customer service, and optimized product offerings tailored to the specific needs of different customer segments, ultimately driving higher customer satisfaction and business performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvVMjpAS7ZcR"
      },
      "outputs": [],
      "source": [
        "df_pca.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prnDK_3V7ZcR"
      },
      "source": [
        "### 3.3.2 - Outlier detection on top of PCs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9Er1jPH7ZcR"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "for col in [\"pc1\", \"pc2\"]:\n",
        "\n",
        "    print(f\"### Column: {col} ###\")\n",
        "    # Calculate the Z-score for a specific column (e.g., annual_inc)\n",
        "    df_pca[f\"{col}_zscore\"] = np.abs(stats.zscore(df_pca[f\"{col}\"]))\n",
        "\n",
        "    # Flag outliers (Z-score > 3)\n",
        "    outliers = df_pca[df_pca[f\"{col}_zscore\"] > 3]\n",
        "    print(f\"Outliers detected using z-scores: {len(outliers)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MPw_qHZ7ZcS"
      },
      "outputs": [],
      "source": [
        "df_pca.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzeXp5_V7ZcS"
      },
      "source": [
        "### 3.3.3 - Clustering on top of PCs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbV-XgSz7ZcS"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Applying K-Means\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df_pca['cluster-km'] = kmeans.fit_predict(df_pca)\n",
        "df_pca.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mN4wB0j7ZcS"
      },
      "outputs": [],
      "source": [
        "# Visualize the clusters\n",
        "plt.scatter(df_pca['pc1'], df_pca['pc2'], c=df_pca['cluster-km'], cmap='viridis')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('K-Means Clustering')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7O8bLVV7ZcS"
      },
      "outputs": [],
      "source": [
        "df_pca.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVPbMkFk7ZcS"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCRp5VWa7ZcS"
      },
      "outputs": [],
      "source": [
        "df_pca.index = df.index\n",
        "df_pca.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "LDhdEHuM7ZcS"
      },
      "outputs": [],
      "source": [
        "df_merged = pd.merge(df_pca[[\"pc1\", \"pc2\", \"cluster-km\"]]\n",
        "                     , df\n",
        "                     , how = \"inner\"\n",
        "                    , left_index=True\n",
        "                    , right_index=True)\n",
        "df_merged.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfrJZRfi7ZcS"
      },
      "outputs": [],
      "source": [
        "# Provide key Statistics by cluster\n",
        "pca_cluster_summary = df_merged \\\n",
        "    .groupby([\"cluster-km\"]) \\\n",
        "    .agg(['median'])#, 'median', 'std', 'min', 'max'])\n",
        "print(\"Key stats by cluster:\")\n",
        "\n",
        "# Set float format to display with 2 decimal places\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "pca_cluster_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgEVULaBTJEN"
      },
      "source": [
        "4\\. Assignment: Create a customer segmentation\n",
        "----------------------------------------------------\n",
        "\n",
        "### Objective\n",
        "\n",
        "The goal of this assignment is to segment a credit card customer base into different groups for marketing and customer management purposes.\n",
        "\n",
        "### Dataset\n",
        "\n",
        "Use the following dataset available on Kaggle:\n",
        "1. Title – Customer Credit Card Data\n",
        "2. Author  - n.a., Fatemeh Habibimoghaddam (Owner)\n",
        "3. Source – https://www.kaggle.com/datasets/fhabibimoghaddam/customer-credit-card-data   \n",
        "(alternatively, can be found in `DSIF-course-material/python-material/data/1-raw/dsif8-assignment`)\n",
        "3. License – Attribution 4.0 International (CC BY 4.0)\n",
        "\n",
        "### Instructions\n",
        "-   **Load and Explore the Data:** Start by loading the data and performing basic exploratory data analysis (EDA).\n",
        "-   **Data Preprocessing:** Handle missing values, standardize the data, and prepare it for clustering.\n",
        "-   **Clustering:** Apply clustering techniques like K-Means or Hierarchical Clustering to segment the customers. Optionally, use PCA to reduce dimensionality before clustering.\n",
        "-   **Segmentation and interpretation:** Create meaningful and valid customer segments (at least 2, pick the number that works best for you based on your analysis) that can be leveraged for targeted marketing, customer management, and strategic decision-making.\n",
        "\n",
        "### Submission\n",
        "Submit your Python code and a brief analysis using Markdown comments summarizing your findings and insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34dXg53m7ZcS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9QHi2M97ZcS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twdLpZECTJEO"
      },
      "source": [
        "### Useful references\n",
        "- [Example of CACI segmentation products](https://www.caci.co.uk/datasets/fresco/): can give you some ideas of what a good segmentation may look like\n",
        "- [Nationwide segmentation use case](https://www.caci.co.uk/insights/case-studies/nationwide/)\n",
        "- Some more on customer segmentation: https://www.caci.co.uk/services/data-science-analytics/customer-segmentation/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho6EK9UKTJEQ"
      },
      "source": [
        "# End of session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb0QdoUETJEQ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(filename=f\"{path_python_material}/images/the-end.jpg\", width=500,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXEHwyeTTJEQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}